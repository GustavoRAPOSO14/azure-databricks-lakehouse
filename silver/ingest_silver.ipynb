{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, expr\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synapse Dedicated Pool Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostname      = dbutils.secrets.get(scope=\"key-vault-secrets\", key=\"synapse-dedicated-pool-server-name\")\n",
    "database_name = dbutils.secrets.get(scope=\"key-vault-secrets\", key=\"synapse-dedicated-pool-database-name\")\n",
    "username      = dbutils.secrets.get(scope=\"key-vault-secrets\", key=\"synapse-dedicated-pool-user\")\n",
    "password      = dbutils.secrets.get(scope=\"key-vault-secrets\", key=\"synapse-dedicated-pool-password\")\n",
    "\n",
    "jdbc_url = f\"jdbc:sqlserver://{hostname}.database.windows.net:1433;database={database_name}\"\n",
    "connection_properties = {\n",
    "    \"user\": username,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture Synapse Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vDay                    = dbutils.widgets.get(\"pDay\")\n",
    "vMonth                  = dbutils.widgets.get(\"pMonth\")\n",
    "vYear                   = dbutils.widgets.get(\"pYear\")\n",
    "vContainer              = dbutils.widgets.get(\"pContainer\")\n",
    "vStorageAccountName     = dbutils.widgets.get(\"pStorageAccountName\")\n",
    "vDatabase               = dbutils.widgets.get(\"pDatabase\")\n",
    "vDatabaseName           = dbutils.widgets.get(\"pDatabaseName\")\n",
    "vSchemaName             = dbutils.widgets.get(\"pSchemaName\")\n",
    "vTableName              = dbutils.widgets.get(\"pTableName\")\n",
    "vLayer                  = dbutils.widgets.get(\"pLayer\")\n",
    "vPrimaryKey             = dbutils.widgets.get(\"pPrimaryKey\")\n",
    "vProcessType            = dbutils.widgets.get(\"pProcessType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vDay                    = \"07\"\n",
    "# vMonth                  = \"11\"\n",
    "# vYear                   = \"2024\"\n",
    "# vContainer              = \"raw\"\n",
    "# vStorageAccountName     = \"stgeusproddata01\"\n",
    "# vDatabase               = \"sqlserver\"\n",
    "# vDatabaseName           = \"FCSACESSO\"\n",
    "# vSchemaName             = \"dbo\"\n",
    "# vTableName              = \"TUSU\"\n",
    "# vLayer                  = \"silver\"\n",
    "# vPrimaryKey             = \"USNOMEUSU\"\n",
    "# vProcessType            = \"INCR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify if bronze layer have data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(f\"\"\"SELECT count(*) FROM bronze_{vDatabaseName}.{vTableName}\"\"\")\n",
    "df_count = df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exit command if bronze layer is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_count == 0:\n",
    "    dbutils.notebook.exit(\"Succeeded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(f\"fs.azure.account.key.{vStorageAccountName}.dfs.core.windows.net\",\n",
    "                dbutils.secrets.get(scope = \"key-vault-secrets\", key = \"token-storage-datalake\"))\n",
    "                \n",
    "display(dbutils.fs.ls(f\"abfss://{vContainer}@{vStorageAccountName}.dfs.core.windows.net/{vDatabase}/{vDatabaseName}/{vSchemaName}/{vTableName}/{vYear}/{vMonth}/{vDay}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session (if needed)\n",
    "spark = SparkSession.builder.appName(\"Retrieve Columns\").getOrCreate()\n",
    "\n",
    "# Query to get column names\n",
    "query = f\"\"\"\n",
    "SELECT COLUMN_NAME\n",
    "FROM information_schema.columns\n",
    "WHERE \n",
    "    table_schema = lower('bronze_{vDatabaseName}') AND table_name = lower('{vTableName}') AND COLUMN_NAME not in ('SYS_CHANGE_VERSION', 'SYS_CHANGE_OPERATION', 'SYS_CHANGE_CREATION_VERSION', 'COMMIT_TIME', 'PARTITION_TIME')\n",
    "ORDER BY ordinal_position\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and get the DataFrame\n",
    "columns_df = spark.sql(query)\n",
    "\n",
    "# Collect column names into a list\n",
    "column_names = [row.COLUMN_NAME for row in columns_df.collect()]\n",
    "\n",
    "# Concatenate column names into a single string\n",
    "columns_string = ', '.join(column_names)\n",
    "\n",
    "# Now you can use the columns_string variable\n",
    "print(columns_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the command to update values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Retrieve Columns\").getOrCreate()\n",
    "\n",
    "# Query to get column names\n",
    "query = f\"\"\"\n",
    "SELECT CONCAT('S.',COLUMN_NAME) as COLUMN_NAME\n",
    "FROM information_schema.columns\n",
    "WHERE \n",
    "    table_schema = lower('bronze_{vDatabaseName}') AND table_name = lower('{vTableName}') AND COLUMN_NAME not in ('SYS_CHANGE_VERSION', 'SYS_CHANGE_OPERATION', 'SYS_CHANGE_CREATION_VERSION', 'COMMIT_TIME', 'PARTITION_TIME')\n",
    "ORDER BY ordinal_position\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and get the DataFrame\n",
    "columns_df = spark.sql(query)\n",
    "\n",
    "# Collect column names into a list\n",
    "column_names = [row.COLUMN_NAME for row in columns_df.collect()]\n",
    "\n",
    "# Concatenate column names into a single string\n",
    "columns_string_insert_values = ', '.join(column_names)\n",
    "\n",
    "# Now you can use the columns_string variable\n",
    "print(columns_string_insert_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the command to update values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session (if needed)\n",
    "spark = SparkSession.builder.appName(\"Retrieve Columns\").getOrCreate()\n",
    "\n",
    "# Query to get column names\n",
    "query = f\"\"\"\n",
    "SELECT CONCAT('T.',COLUMN_NAME, ' = ', 'S.',COLUMN_NAME ) as COLUMN_NAME\n",
    "FROM information_schema.columns\n",
    "WHERE \n",
    "    table_schema = lower('bronze_{vDatabaseName}') AND table_name = lower('{vTableName}') AND COLUMN_NAME not in ('SYS_CHANGE_VERSION', 'SYS_CHANGE_OPERATION', 'SYS_CHANGE_CREATION_VERSION', 'COMMIT_TIME', 'PARTITION_TIME') AND column_name not in ( '{vPrimaryKey}' )\n",
    "ORDER BY ordinal_position \n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and get the DataFrame\n",
    "columns_df = spark.sql(query)\n",
    "\n",
    "# Collect column names into a list\n",
    "column_names = [row.COLUMN_NAME for row in columns_df.collect()]\n",
    "\n",
    "# Concatenate column names into a single string\n",
    "columns_string_update_values = ', '.join(column_names)\n",
    "\n",
    "# Now you can use the columns_string variable\n",
    "print(columns_string_update_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the command to compare primary keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the primary key into a list without spaces\n",
    "primary_keys = vPrimaryKey.split(',')\n",
    "\n",
    "# Create the command string\n",
    "command_primary_key = ' AND '.join([f'T.{key} = S.{key}' for key in primary_keys])\n",
    "\n",
    "# Output the result\n",
    "print(command_primary_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create full command to silver layer if processtype is full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_silver = f\"SELECT {columns_string} FROM bronze_{vDatabaseName}.{vTableName} where PARTITION_TIME = (SELECT MAX(PARTITION_TIME) FROM bronze_{vDatabaseName}.{vTableName})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create merge command to silver layer if processtype is incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the final SQL query\n",
    "query_merge = f\"\"\"\n",
    "MERGE INTO silver_{vDatabaseName}.{vTableName} T\n",
    "USING (\n",
    "  WITH TABLE_MERGE AS (\n",
    "    SELECT\n",
    "      ROW_NUMBER() OVER (PARTITION BY {vPrimaryKey} ORDER BY commit_time DESC) AS dense_rank,\n",
    "      *\n",
    "    FROM\n",
    "      bronze_{vDatabaseName}.{vTableName}\n",
    "    WHERE PARTITION_TIME >= DATEADD(DAY, -3, GETDATE())\n",
    "  ),\n",
    "  TABLE_FILTER AS (\n",
    "    SELECT\n",
    "      sys_change_operation,\n",
    "      {columns_string}\n",
    "    FROM\n",
    "      TABLE_MERGE\n",
    "    WHERE\n",
    "      dense_rank = 1\n",
    "  )\n",
    "  SELECT\n",
    "    DISTINCT *\n",
    "  FROM\n",
    "    TABLE_FILTER\n",
    ") S\n",
    "ON {command_primary_key}\n",
    "WHEN MATCHED AND S.sys_change_operation = 'D' THEN \n",
    "  DELETE\n",
    "WHEN MATCHED AND S.sys_change_operation = 'U' THEN\n",
    "  UPDATE SET {columns_string_update_values}\n",
    "WHEN NOT MATCHED AND S.sys_change_operation IN ('I','U') THEN\n",
    "  INSERT ({columns_string})\n",
    "  VALUES ({columns_string_insert_values});\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute load data to silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if vProcessType == 'FULL':\n",
    "    df = spark.sql(query_silver)\n",
    "    df = df.withColumn(\"DATA_CARGA\", F.current_timestamp() - F.expr(\"INTERVAL 3 HOURS\"))\n",
    "else:\n",
    "    spark.sql(query_merge)\n",
    "    spark.sql(f\"\"\"UPDATE silver_{vDatabaseName}.{vTableName}  SET DATA_CARGA = DATEADD(HOUR, -3, GETDATE())\"\"\")\n",
    "    df = spark.sql(f\"\"\"SELECT * FROM silver_{vDatabaseName}.{vTableName}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataframe to delta table inside raw layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.write.format(\"delta\").partitionBy(\"PARTITION_TIME\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{vLayer}_{vDatabaseName}.{vTableName}\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{vLayer}_{vDatabaseName}.{vTableName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display lines inside delta table of raw layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"SELECT * FROM {vLayer}_{vDatabaseName}.{vTableName} LIMIT 10\"\n",
    "display_query = spark.sql(query)\n",
    "display(display_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write dataframe to container silver inside storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = f\"abfss://{vLayer}@{vStorageAccountName}.dfs.core.windows.net/{vDatabase}/{vDatabaseName}/{vSchemaName}/{vTableName}/{vYear}/{vMonth}/{vDay}/\"\n",
    "df.write.mode(\"overwrite\").parquet(f\"{local_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
